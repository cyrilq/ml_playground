def format_token(token):
    token_word = token.split('|')[0]
    token_class = token.split('|')[1]

    if token_word + '\n' in open("Data/query.txt").readlines():
        id_of_token = open("Data/query.txt").readlines().index(token_word + '\n')
    else:
        id_of_token = '?'
    if token_class + '\n' in open("Data/slots.txt").readlines():
        _class = token_class
        id_of_class = open("Data/slots.txt").readlines().index(token_class + '\n')
    else:
        _class, id_of_class = 0, len(open("Data/slots.txt").readlines()) - 1
    return id_of_token, token_word, id_of_class, _class


def create_token_line(_id, id_of_token, token, id_of_class, _class):
    return "{0:4}|SO {1:3}:1 |# {2:31}|S2 {3:3}:1|# {4}".format(_id, id_of_token, token, id_of_class, _class)


#sentence = 'расходы|S рязанская|L-location область|O на|O образование|S-education в|O 2016|T'


def tokenize(sentence_string, id_):
    result_multiline_string = ''
    bos_string = '{:4}|S0   1:1 |# BOS                              |S2  32:1|# 0\n'.format(id_)
    eos_string = '{:4}|S0   2:1 |# EOS                              |S2  32:1|# 0\n'.format(id_)
    tokens = sentence_string.split(' ')
    for token in tokens:
        result_multiline_string += create_token_line(id_, *format_token(token)) + '\n'

    return bos_string + result_multiline_string + eos_string


i = 1
for string in open('sent.txt', mode='r').readlines():
    print(tokenize(string, i))
    i += 1
