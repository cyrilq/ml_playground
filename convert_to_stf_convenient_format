def format_token(token):
    token_word = token.split('|')[0]
    if len(token.split('|')) > 1:
        token_class = token.split('|')[1]
    else:
        token_class = ''
    if token_word + '\n' in open("Data/query.txt").readlines():
        id_of_token = open("Data/query.txt").readlines().index(token_word + '\n')
    else:
        id_of_token = '?'
    if token_class + '\n' in open("Data/slots.txt").readlines():
        _class = token_class
        id_of_class = open("Data/slots.txt").readlines().index(token_class + '\n')
    else:
        _class, id_of_class = 0, len(open("Data/slots.txt").readlines()) - 1
    return id_of_token, token_word, id_of_class, _class


def create_token_line(_id, id_of_token, token, id_of_class, _class):
    return "{0:4}|SO {1:3}:1 |# {2:31}|S2 {3:3}:1|# {4}".format(_id, id_of_token, token, id_of_class, _class)


sentence = 'расходы|S рязанская|L-location область на образование|S-education в 2016|T'


def tokenize(sentence_string):
    result_multiline_string = ''
    bos_string = '{id_:4}|S0   1:1 |# BOS                              |S2  32:1|# 0\n'.format(id_=0)
    eos_string = '{id_:4}|S0   2:1 |# EOS                              |S2  32:1|# 0\n'.format(id_=0)
    tokens = sentence_string.split(' ')
    for token in tokens:
        result_multiline_string += create_token_line(0, *format_token(token)) + '\n'

    return bos_string + result_multiline_string + eos_string


print(tokenize(sentence))
